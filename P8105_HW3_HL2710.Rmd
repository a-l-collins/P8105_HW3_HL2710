---
title: "P8105_HW3_HL2710"
author: "Ainsel Levitskaia-Collins, HL2710"
date: "2025-10-08"
output: github_document
---

### Preliminary Setup

Adjusting settings such that figures and graphs will be readable:

```{r message = FALSE}
library(tidyverse)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1

Loading the dataset that will be used in this problem:

```{r}
library(p8105.datasets)
data("instacart")
```

The `instacart` dataset has `r nrow(instacart)` observations of `r ncol(instacart)` variables. The variables include information such as which user made the order (`user_id`), which items are in the cart (`product_id`), whether these items are being reordered (`reordered`), the name of the product (`product_name`), and other such information. For example, the first item added to user 112108's cart was Bulgarian Yogurt with a product ID of 49302, and this is a reordered item.

#### Determining Aisle Information

First, producing a dataset with number of items ordered per aisle:

```{r}
instacart_aislecounts <- instacart %>% 
  count(aisle, name = "num_items_ordered") %>% 
  arrange(desc(num_items_ordered))
```

Then, selecting the top 5 most popular aisles:

```{r}
instacart_aislecounts %>% slice_head(n = 5)
```

There are `r nrow(instacart_aislecounts)` aisles within the `instacart` dataset, and the top 5 aisles that the most items are ordered from are fresh vegetables, fresh fruits, packaged vegetables and fruits, yogurt, and packaged cheese.

#### Number of Ordered Items per Aisle

Creating a plot that shows the number of items ordered per aisle, limited to only aisles with more than 10,000 items ordered:

```{r}
instacart_aislecounts %>% 
  filter(num_items_ordered > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, desc(num_items_ordered))) %>% 
  ggplot(aes(x = aisle, y = num_items_ordered)) +
  geom_bar(stat = "identity", fill = "chartreuse3", width = .4) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  xlab("") +
  ylab("Number of Items Ordered")
```

#### Table of Most Popular Items

Creating a table of the three most popular items in the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

1. Keeping only the desired aisles
1. Counting by `product_name` after grouping by `aisle` and `product_name` so that every product has an associated number of times it was ordered
1. Keeping only the three most popular items per aisle
1. Converting from long to wide format for readability

First, creating a database with information on aisle name, product name, and how many times each item has been ordered:

```{r}
instacart_popular <- instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name, name = "num_items_ordered") %>% 
  arrange(desc(num_items_ordered)) %>% 
  filter(min_rank(desc(num_items_ordered)) < 4)
```

Creating a table to display this information:

```{r}
knitr::kable(
  instacart_popular,
  col.names = c("Aisle", "Product Name", "Number of Items Ordered")
)
```


### Table of Mean hr Day per Week when Pink Lady Apples and Coffee Ice Cream are Ordered

Creating a table in which, for every day of the week, the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered is displayed.

1. Keeping only the desired products
1. Grouping by day of week
1. Creating a mean time of order variable
1. Using `pivot_wider` to make the data human-readable

I would like to note that while under normal circumstances, I would have renamed the `order_dow` contents from 0-6 to corresponding days of the week, I could not find any confirmation online for whether 0 corresponded to Monday, Sunday, or something else, and have as such chosen to leave `order_dow` in its numerical format.

```{r warning = FALSE, message = FALSE}
instacart_times <- instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_time = mean(order_hour_of_day, na.rm = TRUE)) %>% 
  pivot_wider(
    names_from = "product_name",
    values_from = "mean_time"
  )
```

Creating a table to display this information:

```{r}
knitr::kable(
  instacart_times,
  col.names = c("Day of Week", "Coffee Ice Cream (military time)", "Pink Lady Apples (military time)"),
  digits = 0
)
```


### Problem 2

Importing and cleaning the Zillow dataset, based on what was done in Homework 2:

Importing and cleaning the `zori` dataset:

1. Importing and cleaning
1. Renaming `region_name` to `zip_code` to match the `zipcodes` dataset, and moving that variable to the front
1. Converting the dates and rental prices from wide to long format
1. Adjusting `county_name` to match the corresponding variable in the `zipcodes` dataset
1. Renaming the `county` variable so that both the name and contents reflect the NYC boroughs
1. Cleaning the dates so that year, month, and day are separate variables and appropriately named

```{r message = FALSE}
zori <-
  read_csv("./data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") %>% 
  janitor::clean_names() %>% 
  select(zip_code = region_name, everything()) %>% 
  separate(county_name, into = c("county", "extra"), sep = " C") %>% 
  select(-extra, -state_name)

zori_neat <-
  pivot_longer(
    zori,
    x2015_01_31:x2024_08_31,
    names_to = "date",
    names_prefix = "x",
    values_to = "rental_price") %>% 
  mutate(
    county = case_match(county,
      "Kings" ~ "Brooklyn",
      "New York" ~ "Manhattan",
      "Richmond" ~ "Staten Island",
      "Bronx" ~ "Bronx",
      "Queens" ~ "Queens")
  ) %>% 
  rename(borough = county) %>% 
  separate_wider_delim(date, names = c("year", "month", "day"), delim = "_") %>% 
  mutate(
    month = case_match(month,
      "01" ~ "January",
      "02" ~ "February",
      "03" ~ "March",
      "04" ~ "April",
      "05" ~ "May",
      "06" ~ "June",
      "07" ~ "July",
      "08" ~ "August",
      "09" ~ "September",
      "10" ~ "October",
      "11" ~ "November",
      "12" ~ "December"))
```

#### Zip Code Counts

```{r}
zip_counts <- zori_neat %>%
  drop_na(rental_price) %>% 
  count(zip_code, name = "times_observed")
```

There are `r sum(pull(zip_counts, times_observed) > 115)` zip codes observed 116 times within the Zillow `zori` dataset, and there are `r sum(pull(zip_counts, times_observed) < 10)` zip codes observed less than 10 times within the Zillow `zori` dataset. Zillow is a website that both manages renting and selling of properties, and it is not the only website used within NYC to manage renting properties (StreetEasy being an alternative option). As such, the zip codes which appear more often within the Zillow dataset, such as 10001 or 10003, are in popular locations to live within NYC, whereas zip codes that appear less often, such as 10453, are in less popular locations to live within NYC, which likely reflects the demographic that tend to use Zillow, both from a landlord and renter perspective.

#### Average Rental Price per Borough and Year

Creating a database with the average rental price per year for each borough:

1. Grouping by borough and year
1. Finding the mean price for each year, per borough
1. Converting to wide format for readability

```{r message = FALSE}
avg_price_year <- zori_neat %>% 
  group_by(borough, year) %>% 
  summarize(mean_price = mean(rental_price, na.rm = TRUE)) %>% 
  pivot_wider(
    names_from = "borough",
    values_from = "mean_price"
  )
```

Creating a table out of the database:

```{r}
options(knitr.kable.NA = "---")
knitr::kable(
  avg_price_year,
  digits = 1
)
```

Within this table, we can see that all boroughs have had an increase in rental price over the past 9 years. Queens has had the smallest overall rent increase, despite Staten Island not having data until 2020, and Manhattan has had the largest overall rent increase.

#### Borough Rental Price Comparison across All Years

Creating a dataset with the average rental price per borough, across all years:

```{r warning = FALSE}
avg_price_all_plot <- zori_neat %>% 
  ggplot(aes(x = borough, y = rental_price)) +
  geom_boxplot() +
  labs(
    title = "Borough Prices Across All Years",
    x = "Borough",
    y = "Rental Price"
  )

avg_price_all_plot
```

Within this plot, the borough with the largest variation in rental prices is Manhattan, while the borough with the smallest variation in rental prices is Staten Island. The borough with the largest median rental price is Manhattan, while the borough with the smallest median rental price is the Bronx. All boroughs have outlier prices that are higher than the main quartiles, while none have outlier prices that are lower than the main quartiles.

#### Borough Rental Price Comparison across 2023

```{r warning = FALSE}
avg_price_month_plot <- zori_neat %>% 
  filter(year == 2023) %>% 
  ggplot(aes(x = borough, y = rental_price)) +
  geom_boxplot() +
  labs(
    title = "Borough Prices Across 2023",
    x = "Borough",
    y = "Rental Price"
  )

avg_price_month_plot
```

Within this plot, the borough with the largest variation in rental prices is Manhattan, and the borough with the smallest variation in rental prices is Staten Island. The Bronx, Staten Island, and Queens all have relatively similar and low median rental prices, while Manhattan's is the largest. The overall distribution of the five boroughs is similar in 2023 compared to the plot of all years, but there is more exaggeration of the differences between the boroughs within the 2023 plot, compared to the plot of all years.

#### Combine Plots into a Single Graphic and Export

This is done using the `patchwork` package, which was loaded in the initial setup code block:

```{r warning = FALSE}
avg_price_plots <- avg_price_all_plot + avg_price_month_plot

avg_price_plots

ggsave("./results/avg_prices.pdf", avg_price_plots, width = 8, height = 5)
```

### Problem 3

#### Importing and Cleaning Data

Importing and cleaning the `demographics` dataset:

1. Import and clean names
1. Clean the `sex` variable (rename contents to be non-numeric, convert to factor)
1. Clean the `education` variable (rename contents to be non-numeric, convert to factor)
1. Remove all with missing data
1. Remove any less than 21 years old

```{r message = FALSE}
demographics <-
  read_csv("./data/nhanes_covar.csv", skip = 4) %>% 
  janitor::clean_names() %>% 
  mutate(
    sex = case_match(sex,
      1 ~ "Male",
      2 ~ "Female"),
    sex = factor(sex),
    education = case_match(education,
      1 ~ "Less than high school",
      2 ~ "High school equivalent",
      3 ~ "More than high school"),
    education = factor(education)) %>% 
  drop_na() %>% 
  filter(age > 20)

demographics
```


Importing and cleaning the `accelerometer` dataset:

1. Import and clean names
1. Convert from wide to long format (continued on a newline to prevent an R crash)

```{r message = FALSE}
accelerometer <-
  read_csv("./data/nhanes_accel.csv") %>% 
  janitor::clean_names()

accelerometer <-
  pivot_longer(
    data = accelerometer,
    cols = min1:min1440,
    names_to = "min",
    names_prefix = "min",
    values_to = "mims"
  )

accelerometer
```


Merging the `demographics` and `accelerometer` datasets:

- Using `inner_join` so that only those participants in the `accelerometer` dataset with demographic data will be included

```{r}
accel_dem <- inner_join(accelerometer, demographics, by = "seqn")

accel_dem
```

